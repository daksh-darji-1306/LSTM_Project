{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "FTlAEuUoHd5q",
        "outputId": "53b904e5-c517-4fcf-ab8b-ae0107d06502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Building the model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the model...\n",
            "Epoch 1/10\n",
            "668/668 - 26s - 39ms/step - accuracy: 0.7361 - loss: 0.5028 - val_accuracy: 0.8499 - val_loss: 0.3740\n",
            "Epoch 2/10\n",
            "668/668 - 21s - 32ms/step - accuracy: 0.8943 - loss: 0.2764 - val_accuracy: 0.8478 - val_loss: 0.3636\n",
            "Epoch 3/10\n",
            "668/668 - 41s - 61ms/step - accuracy: 0.9270 - loss: 0.2018 - val_accuracy: 0.8560 - val_loss: 0.3776\n",
            "Epoch 4/10\n",
            "668/668 - 22s - 33ms/step - accuracy: 0.9429 - loss: 0.1629 - val_accuracy: 0.8480 - val_loss: 0.4420\n",
            "Epoch 5/10\n",
            "668/668 - 23s - 35ms/step - accuracy: 0.9535 - loss: 0.1359 - val_accuracy: 0.8441 - val_loss: 0.5829\n",
            "Epoch 6/10\n",
            "668/668 - 22s - 33ms/step - accuracy: 0.9618 - loss: 0.1168 - val_accuracy: 0.8401 - val_loss: 0.5371\n",
            "Epoch 7/10\n",
            "668/668 - 22s - 33ms/step - accuracy: 0.9676 - loss: 0.1027 - val_accuracy: 0.8396 - val_loss: 0.4192\n",
            "Epoch 8/10\n",
            "668/668 - 23s - 35ms/step - accuracy: 0.9722 - loss: 0.0900 - val_accuracy: 0.8340 - val_loss: 0.6193\n",
            "Epoch 9/10\n",
            "668/668 - 23s - 34ms/step - accuracy: 0.9787 - loss: 0.0743 - val_accuracy: 0.8328 - val_loss: 0.7387\n",
            "Epoch 10/10\n",
            "668/668 - 22s - 32ms/step - accuracy: 0.9803 - loss: 0.0664 - val_accuracy: 0.8282 - val_loss: 0.7413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving the model and tokenizer...\n",
            "\n",
            "Training complete. 'sarcasm_model.h5' and 'tokenizer.json' have been saved.\n"
          ]
        }
      ],
      "source": [
        "# --- Sarcasm Detection Model Training Script ---\n",
        "# This script loads the dataset, preprocesses the data, trains the LSTM model,\n",
        "# and then saves the trained model and the tokenizer to files.\n",
        "# Run this script once to prepare the assets for the Streamlit app.\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Step 1: Load the Dataset ---\n",
        "file_path = 'Sarcasm_Headlines_Dataset.json'\n",
        "data = []\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# --- Step 2: Data Preprocessing ---\n",
        "# Hyperparameters\n",
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 40\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences = df['headline'].tolist()\n",
        "labels = np.array(df['is_sarcastic'].tolist())\n",
        "\n",
        "# Split data to ensure the tokenizer is only fit on training data\n",
        "train_sentences, _, _, _ = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the training text\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "# Convert all sentences to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Split the processed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Step 3: Build the LSTM Model ---\n",
        "print(\"\\nBuilding the model...\")\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# --- Step 4: Train the Model ---\n",
        "print(\"\\nTraining the model...\")\n",
        "num_epochs = 10\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# --- Step 5: Save the Model and Tokenizer ---\n",
        "print(\"\\nSaving the model and tokenizer...\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"sarcasm_model.h5\")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open(\"tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "print(\"\\nTraining complete. 'sarcasm_model.h5' and 'tokenizer.json' have been saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "modt4pQZIs4i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}